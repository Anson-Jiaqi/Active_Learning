{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S9BoR514bae"
   },
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnzrsVhF40E-"
   },
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1744,
     "status": "ok",
     "timestamp": 1730350106892,
     "user": {
      "displayName": "孙嘉淇",
      "userId": "08710618805620589707"
     },
     "user_tz": -600
    },
    "id": "6ozfLCPE45zY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "training_set = pd.read_csv('cybersecurity_training.csv',sep=\"|\")\n",
    "testing_set = pd.read_csv('cybersecurity_test.csv',sep=\"|\")\n",
    "\n",
    "\n",
    "training_set = training_set.set_index('alert_ids')\n",
    "testing_set = testing_set.set_index('alert_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('cybersecurity_test_targets.txt', 'r') as f:\n",
    "  test_targets = f.readlines()\n",
    "test_targets = [int(target.strip()) for target in test_targets]\n",
    "\n",
    "testing_set['notified'] = test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.drop(\"client_code\",axis=1)\n",
    "testing_set = testing_set.drop(\"client_code\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.fillna(-1)\n",
    "testing_set = testing_set.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of columns to convert to categorical\n",
    "categorical_columns = [\"categoryname\", \"ipcategory_name\",\"ip\", \"ipcategory_scope\", \"parent_category\",\n",
    "                       \"grandparent_category\", \"weekday\", \"isiptrusted\", \"enforcementscore\",\n",
    "                       \"dstipcategory_dominate\", \"srcipcategory_dominate\", \"dstportcategory_dominate\",\n",
    "                       \"srcportcategory_dominate\",\"notified\"]\n",
    "\n",
    "# Convert specified columns to categorical type in training set\n",
    "for col in categorical_columns:\n",
    "  if col in training_set.columns:\n",
    "    training_set[col] = training_set[col].astype('category')\n",
    "\n",
    "# Convert specified columns to categorical type in testing set\n",
    "for col in categorical_columns:\n",
    "  if col in testing_set.columns:\n",
    "    testing_set[col] = testing_set[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set.drop(\"notified\", axis=1)\n",
    "Y = training_set[\"notified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X\n",
    "y_train_full = Y\n",
    "X_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = testing_set.drop(\"notified\", axis=1)\n",
    "test_y = testing_set[\"notified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "    tree_method=\"auto\",\n",
    "    enable_categorical=True,\n",
    "    max_cat_to_onehot=1,\n",
    "    device=\"cuda\",\n",
    "    booster=\"gbtree\",\n",
    "    sampling_method=\"uniform\",\n",
    "    subsample=1,\n",
    "    reg_lambda=1,\n",
    "    alpha=10,\n",
    "    scale_pos_weight=17,  # sum(negative instances) / sum(positive instances)\n",
    "    process_type=\"default\",\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    seed=42,\n",
    "    colsample_bytree=0.6,\n",
    "    learning_rate=0.001,\n",
    "    max_depth=6,\n",
    "    n_estimators=2000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\3.10.6\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:51:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8257024795294661"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)\n",
    "probs_test = clf.predict_proba(test_X)\n",
    "auc_score = roc_auc_score(test_y, probs_test[:, 1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 8632,
     "status": "ok",
     "timestamp": 1730349974764,
     "user": {
      "displayName": "孙嘉淇",
      "userId": "08710618805620589707"
     },
     "user_tz": -600
    },
    "id": "YWbsXvrH4FYc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "training_set = pd.read_csv('train_onehot.csv')\n",
    "testing_set = pd.read_csv('test_onehot.csv')\n",
    "\n",
    "training_set = training_set.set_index('alert_ids')\n",
    "testing_set = testing_set.set_index('alert_ids')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set.drop(\"notified\", axis=1)\n",
    "Y = training_set[\"notified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X\n",
    "y_train_full = Y\n",
    "X_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = testing_set.drop(\"notified\", axis=1)\n",
    "test_y = testing_set[\"notified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1056,
     "status": "ok",
     "timestamp": 1730354726972,
     "user": {
      "displayName": "孙嘉淇",
      "userId": "08710618805620589707"
     },
     "user_tz": -600
    },
    "id": "DCt1Yjf463yT"
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "    tree_method=\"auto\",\n",
    "    enable_categorical=True,\n",
    "    max_cat_to_onehot=1,\n",
    "    device=\"cuda\",\n",
    "    booster=\"gbtree\",\n",
    "    sampling_method=\"uniform\",\n",
    "    subsample=1,\n",
    "    reg_lambda=1,\n",
    "    alpha=10,\n",
    "    scale_pos_weight=17,  # sum(negative instances) / sum(positive instances)\n",
    "    process_type=\"default\",\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    seed=42,\n",
    "    colsample_bytree=0.6,\n",
    "    learning_rate=0.001,\n",
    "    max_depth=6,\n",
    "    n_estimators=2000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2719,
     "status": "ok",
     "timestamp": 1730354730423,
     "user": {
      "displayName": "孙嘉淇",
      "userId": "08710618805620589707"
     },
     "user_tz": -600
    },
    "id": "MmJkfjp549Tz",
    "outputId": "79e7620f-1099-436f-bbef-8c88c179cf78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8809424518509976"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)\n",
    "probs_test = clf.predict_proba(test_X)\n",
    "auc_score = roc_auc_score(test_y, probs_test[:, 1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srLNYryt50-g"
   },
   "source": [
    "# **Active learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTZXpenpKbg7"
   },
   "source": [
    "## **Least confident**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VhNS6S8KfQ7",
    "outputId": "a9c287ff-40db-4c00-8ea1-f56ce3726521"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of random states for reproducibility\n",
    "random_state = [6, 15, 10, 17, 42, 20, 67, 49, 12, 25]\n",
    "auc_scores_total = {}\n",
    "num_samples_per_iter = 1\n",
    "\n",
    "# Loop through each random state\n",
    "for rs in random_state:\n",
    "    # Split the dataset into training and pool sets with a small initial training size\n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X, Y, train_size=0.001, random_state=rs)\n",
    "    auc_scores = []\n",
    "    print(\"random state =\", rs)\n",
    "    \n",
    "    # Active learning loop\n",
    "    for i in range(200):\n",
    "        # Fit the classifier on the current training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict probabilities on the pool set and test set\n",
    "        probs = clf.predict_proba(X_pool)\n",
    "        probs_test = clf.predict_proba(test_X)\n",
    "        \n",
    "        # Calculate the AUC score and append to the list of scores\n",
    "        auc_score = roc_auc_score(test_y, probs_test[:, 1])\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "        print(\"X size:\", len(X_train))\n",
    "        print(\"AUC Score:\", auc_score)\n",
    "        \n",
    "        # Calculate uncertainty of each sample in the pool set\n",
    "        uncertainty = 1 - np.max(probs, axis=1)\n",
    "        \n",
    "        # Select the samples with the highest uncertainty\n",
    "        query_indices = np.argsort(uncertainty)[-num_samples_per_iter:]\n",
    "        query_samples = X_pool.iloc[query_indices]\n",
    "        y_query_samples = y_pool.iloc[query_indices]\n",
    "        \n",
    "        # Add the selected samples to the training set and remove them from the pool set\n",
    "        X_train = pd.concat([X_train, query_samples])\n",
    "        y_train = pd.concat([y_train, y_query_samples])\n",
    "        X_pool = X_pool.drop(query_samples.index)\n",
    "        y_pool = y_pool.drop(query_samples.index)\n",
    "    \n",
    "    # Store the AUC scores for each random state\n",
    "    auc_scores_total[rs] = auc_scores[:]\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5D7O6NxCLhSz"
   },
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(auc_scores_total)\n",
    "auc_df.to_csv('lc_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG52NpVz5tio"
   },
   "source": [
    "## **Margin Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Tx21C2cKRyo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sx2AHK3E5AHQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of random states for reproducibility\n",
    "random_state = [6, 15, 10, 17, 42, 20, 67, 49, 12, 25]\n",
    "num_samples_per_iter = 1\n",
    "auc_scores_total_margin = {}\n",
    "\n",
    "# Loop through each random state\n",
    "for rs in random_state:\n",
    "    # Split the dataset into training and pool sets with a small initial training size\n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X, Y, train_size=0.001, random_state=rs)\n",
    "    auc_scores = []\n",
    "    print(\"random state =\", rs)\n",
    "    \n",
    "    # Active learning loop\n",
    "    for i in range(200):\n",
    "        # Fit the classifier on the current training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict probabilities on the pool set and test set\n",
    "        probs = clf.predict_proba(X_pool)\n",
    "        probs_test = clf.predict_proba(test_X)\n",
    "        \n",
    "        # Calculate the AUC score and append to the list of scores\n",
    "        auc_score = roc_auc_score(test_y, probs_test[:, 1])\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "        print(\"X size:\", len(X_train))\n",
    "        print(\"AUC Score:\", auc_score)\n",
    "        \n",
    "        # Sort probabilities to calculate margins\n",
    "        sorted_probs = np.sort(probs, axis=1)\n",
    "        margins = sorted_probs[:, -1] - sorted_probs[:, -2]\n",
    "        \n",
    "        # Select the samples with the smallest margins\n",
    "        query_indices = np.argsort(margins)[:num_samples_per_iter]\n",
    "        query_samples = X_pool.iloc[query_indices]\n",
    "        y_query_samples = y_pool.iloc[query_indices]\n",
    "        \n",
    "        # Add the selected samples to the training set and remove them from the pool set\n",
    "        X_train = pd.concat([X_train, query_samples])\n",
    "        y_train = pd.concat([y_train, y_query_samples])\n",
    "        X_pool = X_pool.drop(query_samples.index)\n",
    "        y_pool = y_pool.drop(query_samples.index)\n",
    "    \n",
    "    # Store the AUC scores for each random state\n",
    "    auc_scores_total_margin[rs] = auc_scores[:]\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSuwSkCK6AJi"
   },
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(auc_scores_total_margin)\n",
    "auc_df.to_csv('ms_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2s-cN727Qxl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGg-EsDWLyjX"
   },
   "source": [
    "## **entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcJibDLZL14L"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# List of random states for reproducibility\n",
    "random_state = [6, 15, 10, 17, 42, 20, 67, 49, 12, 25]\n",
    "auc_scores_total_entropy = {}\n",
    "num_samples_per_iter = 1\n",
    "\n",
    "# Loop through each random state\n",
    "for rs in random_state:\n",
    "    # Split the dataset into training and pool sets with a small initial training size\n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X, Y, train_size=0.001, random_state=rs)\n",
    "    auc_scores = []\n",
    "    \n",
    "    # Active learning loop\n",
    "    for i in range(200):\n",
    "        # Fit the classifier on the current training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict probabilities on the pool set and test set\n",
    "        probs = clf.predict_proba(X_pool)\n",
    "        probs_test = clf.predict_proba(test_X)\n",
    "        \n",
    "        # Calculate the AUC score and append to the list of scores\n",
    "        auc_score = roc_auc_score(test_y, probs_test[:, 1])\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "        print(\"AUC Score:\", auc_score)\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        probs_normalized = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Calculate entropies (avoid log(0))\n",
    "        entropies = -np.sum(probs_normalized * np.log(probs_normalized + 1e-10), axis=1)\n",
    "        \n",
    "        # Select the samples with the highest entropy\n",
    "        query_indices = np.argsort(entropies)[-num_samples_per_iter:]\n",
    "        query_samples = X_pool.iloc[query_indices]\n",
    "        y_query_samples = y_pool.iloc[query_indices]\n",
    "        \n",
    "        # Add the selected samples to the training set and remove them from the pool set\n",
    "        X_train = pd.concat([X_train, query_samples])\n",
    "        y_train = pd.concat([y_train, y_query_samples])\n",
    "        X_pool = X_pool.drop(query_samples.index)\n",
    "        y_pool = y_pool.drop(query_samples.index)\n",
    "    \n",
    "    # Store the AUC scores for each random state\n",
    "    auc_scores_total_entropy[rs] = auc_scores[:]\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0bAaxXWL7Eg"
   },
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(auc_scores_total_entropy)\n",
    "auc_df.to_csv('entropy_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbWlRcFA80Jo"
   },
   "source": [
    "# **Density-Weighted**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjglkIkK_onX"
   },
   "source": [
    "## **Density-entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWU922VR9R_U"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of random states for reproducibility\n",
    "random_state = [6, 15, 10, 17, 42, 20, 67, 49, 12, 25]\n",
    "auc_scores_total_density_entropy = {}\n",
    "num_samples_per_iter = 1\n",
    "\n",
    "# Function to calculate density\n",
    "def calculate_density(X):\n",
    "    nbrs = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    density = np.mean(distances, axis=1)\n",
    "    return density\n",
    "\n",
    "# Loop through each random state\n",
    "for rs in random_state:\n",
    "    # Split the dataset into training and pool sets with a small initial training size\n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X, Y, train_size=0.001, random_state=rs)\n",
    "    auc_scores = []\n",
    "    densities = calculate_density(X_pool)\n",
    "    valid_mask = np.ones(len(X_pool), dtype=bool)\n",
    "    \n",
    "    # Active learning loop\n",
    "    for i in range(200):\n",
    "        # Fit the classifier on the current training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict probabilities on the pool set and test set\n",
    "        probs = clf.predict_proba(X_pool)\n",
    "        probs_test = clf.predict_proba(test_X)\n",
    "        \n",
    "        # Calculate the AUC score and append to the list of scores\n",
    "        auc_score = roc_auc_score(test_y, probs_test[:, 1])\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "        print(\"AUC Score:\", auc_score)\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        probs_normalized = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Calculate entropies (avoid log(0))\n",
    "        entropies = -np.sum(probs_normalized * np.log(probs_normalized + 1e-10), axis=1)\n",
    "        \n",
    "        # Calculate weighted entropies\n",
    "        weighted_entropies = entropies[valid_mask] / densities[valid_mask]\n",
    "        \n",
    "        # Select the samples with the highest weighted entropies\n",
    "        query_indices = np.argsort(weighted_entropies)[-num_samples_per_iter:]\n",
    "        queried_indices = np.where(valid_mask)[0][query_indices]\n",
    "        query_samples = X_pool.iloc[queried_indices]\n",
    "        y_query_samples = y_pool.iloc[queried_indices]\n",
    "        \n",
    "        # Add the selected samples to the training set and remove them from the pool set\n",
    "        X_train = pd.concat([X_train, query_samples])\n",
    "        y_train = pd.concat([y_train, y_query_samples])\n",
    "        valid_mask[queried_indices] = False\n",
    "    \n",
    "    X_pool = X_pool[valid_mask]\n",
    "    y_pool = y_pool[valid_mask]\n",
    "    # Store the AUC scores for each random state\n",
    "    auc_scores_total_density_entropy[rs] = auc_scores[:]\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIczDaBI9oN_"
   },
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(auc_scores_total_density_entropy)\n",
    "auc_df.to_csv('de_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4xosFdD9xFx"
   },
   "source": [
    "## **Density-lc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9PMuqjh9wP_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of random states for reproducibility\n",
    "random_state = [6, 15, 10, 17, 42, 20, 67, 49, 12, 25]\n",
    "auc_scores_total_density_least_confident = {}\n",
    "num_samples_per_iter = 1\n",
    "\n",
    "# Function to calculate density\n",
    "def calculate_density(X):\n",
    "    nbrs = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    density = np.mean(distances, axis=1)\n",
    "    return density\n",
    "\n",
    "# Loop through each random state\n",
    "for rs in random_state:\n",
    "    # Split the dataset into training and pool sets with a small initial training size\n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X, Y, train_size=0.001, random_state=rs)\n",
    "    auc_scores = []\n",
    "    print(\"random state =\", rs)\n",
    "    densities = calculate_density(X_pool)\n",
    "    valid_mask = np.ones(len(X_pool), dtype=bool)\n",
    "    \n",
    "    # Active learning loop\n",
    "    for i in range(200):\n",
    "        # Fit the classifier on the current training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict probabilities on the pool set and test set\n",
    "        probs = clf.predict_proba(X_pool)\n",
    "        probs_test = clf.predict_proba(test_X)\n",
    "        \n",
    "        # Calculate the AUC score and append to the list of scores\n",
    "        auc_score = roc_auc_score(test_y, probs_test[:, 1])\n",
    "        auc_scores.append(auc_score)\n",
    "        print(\"AUC Score:\", auc_score)\n",
    "        \n",
    "        # Calculate least confident scores\n",
    "        least_confident = 1 - np.max(probs, axis=1)\n",
    "        \n",
    "        # Calculate weighted least confident scores\n",
    "        weighted_least_confident = least_confident[valid_mask] / densities[valid_mask]\n",
    "        \n",
    "        # Select the samples with the highest weighted least confident scores\n",
    "        query_indices = np.argsort(weighted_least_confident)[-num_samples_per_iter:]\n",
    "        queried_indices = np.where(valid_mask)[0][query_indices]\n",
    "        query_samples = X_pool.iloc[queried_indices]\n",
    "        y_query_samples = y_pool.iloc[queried_indices]\n",
    "        \n",
    "        # Add the selected samples to the training set and remove them from the pool set\n",
    "        X_train = pd.concat([X_train, query_samples])\n",
    "        y_train = pd.concat([y_train, y_query_samples])\n",
    "        valid_mask[queried_indices] = False\n",
    "    \n",
    "    # Update the pool set with valid samples\n",
    "    X_pool = X_pool[valid_mask]\n",
    "    y_pool = y_pool[valid_mask]\n",
    "    \n",
    "    # Store the AUC scores for each random state\n",
    "    auc_scores_total_density_least_confident[rs] = auc_scores[:]\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djDRb4OB9_BK"
   },
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(auc_scores_total_density_lest_confident)\n",
    "auc_df.to_csv('dlc_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrImmxFo_TFa"
   },
   "source": [
    "## **Density-margin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57Apch15-DRv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of random states for reproducibility\n",
    "random_state = [6, 15, 10, 17, 42, 20, 67, 49, 12, 25]\n",
    "auc_scores_total_density_margin = {}\n",
    "num_samples_per_iter = 1\n",
    "\n",
    "# Function to calculate density\n",
    "def calculate_density(X):\n",
    "    nbrs = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    density = np.mean(distances, axis=1)\n",
    "    return density\n",
    "\n",
    "# Loop through each random state\n",
    "for rs in random_state:\n",
    "    # Split the dataset into training and pool sets with a small initial training size\n",
    "    X_train, X_pool, y_train, y_pool = train_test_split(X, Y, train_size=0.001, random_state=rs)\n",
    "    print(\"random state =\", rs)\n",
    "    auc_scores = []\n",
    "    densities = calculate_density(X_pool)\n",
    "    valid_mask = np.ones(len(X_pool), dtype=bool)\n",
    "    \n",
    "    # Active learning loop\n",
    "    for i in range(200):\n",
    "        # Fit the classifier on the current training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict probabilities on the pool set and test set\n",
    "        probs = clf.predict_proba(X_pool)\n",
    "        probs_test = clf.predict_proba(test_X)\n",
    "        \n",
    "        # Calculate the AUC score and append to the list of scores\n",
    "        auc_score = roc_auc_score(test_y, probs_test[:, 1])\n",
    "        auc_scores.append(auc_score)\n",
    "        print(\"AUC Score:\", auc_score)\n",
    "        \n",
    "        # Sort probabilities to calculate margins\n",
    "        sorted_probs = np.sort(probs, axis=1)\n",
    "        margin = sorted_probs[:, -1] - sorted_probs[:, -2]\n",
    "        \n",
    "        # Calculate weighted margins\n",
    "        weighted_margin = margin[valid_mask] / densities[valid_mask]\n",
    "        \n",
    "        # Select the samples with the smallest margins\n",
    "        query_indices = np.argsort(weighted_margin)[:num_samples_per_iter]\n",
    "        queried_indices = np.where(valid_mask)[0][query_indices]\n",
    "        query_samples = X_pool.iloc[queried_indices]\n",
    "        y_query_samples = y_pool.iloc[queried_indices]\n",
    "        \n",
    "        # Add the selected samples to the training set and remove them from the pool set\n",
    "        X_train = pd.concat([X_train, query_samples])\n",
    "        y_train = pd.concat([y_train, y_query_samples])\n",
    "        valid_mask[queried_indices] = False\n",
    "    \n",
    "    # Update the pool set with valid samples\n",
    "    X_pool = X_pool[valid_mask]\n",
    "    y_pool = y_pool[valid_mask]\n",
    "    \n",
    "    # Store the AUC scores for each random state\n",
    "    auc_scores_total_density_margin[rs] = auc_scores[:]\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWnuB576_b6o"
   },
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(auc_scores_total_density_margin)\n",
    "auc_df.to_csv('dm_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugyyQoL9_e38"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPrzJePA2cIbqoFAx5grIbk",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1bXj1H23kODswN6PNOLutT1yykMX_Svrc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
